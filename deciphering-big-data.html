<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <title>Deciphering Big Data</title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
    <style>
      html, body {
        width: 100%;
        height: 100%;
        min-height: 100%;
        -webkit-text-size-adjust: 100%;
        -ms-text-size-adjust: 100%;
        margin: 0;
        padding: 0;
        overflow-x: hidden;
      }
      *, *::before, *::after { box-sizing: border-box; }
      #wrapper, #main, .article-wrapper {
        width: 100%;
        max-width: 100%;
        box-sizing: border-box;
      }
      #main {
        padding: 3.5rem clamp(.75rem, 4vw, 2rem);
        margin: 0 auto;
      }
      .article-wrapper {
        opacity: 0;
        transform: translateY(12px);
        transition: opacity 520ms ease, transform 520ms ease;
        position: relative;
        width: 100%;
        max-width: 980px;
        margin: 0 auto;
        padding: clamp(0.75rem, 4vw, 2rem);
        overflow-x: hidden;
        box-sizing: border-box;
      }
      .article-wrapper.visible { opacity: 1; transform: none; }
      .article-wrapper .close {
        position: absolute;
        right: clamp(0.75rem, 4vw, 2rem);
        top: clamp(0.6rem, 2.5vw, 1.4rem);
        display: inline-block;
        color: #fff;
        background: rgba(0,0,0,0.35);
        width: 36px;
        height: 36px;
        line-height: 36px;
        text-align: center;
        border-radius: 50%;
        text-decoration: none;
        font-weight: 700;
        transition: background .2s ease, transform .2s ease;
        z-index: 5;
      }
      .article-wrapper .close:hover { background: rgba(0,0,0,0.55); transform: translateY(-2px); }
      h2.major {
        margin: 0 0 1rem 0;
        line-height: 1.05;
        font-weight: 700;
        font-size: clamp(1.05rem, 3.2vw, 1.25rem);
      }
      .image.main, .image.main img {
        display: block;
        max-width: 100%;
        height: auto;
        margin: 0 auto 1rem;
      }
      .image.main img { width: 100%; height: auto; display: block; }
      #bg {
        position: fixed;
        inset: 0;
        z-index: -2;
        background: linear-gradient(180deg, rgba(12,34,56,0.85) 0%, rgba(28,45,70,0.9) 60%), url('images/bg-noise.png');
        background-size: cover, 400px 400px;
        pointer-events: none;
      }
      #bg::after {
        content: "";
        position: absolute;
        inset: 0;
        background: radial-gradient(ellipse at center, rgba(0,0,0,0) 40%, rgba(0,0,0,0.25) 100%);
        pointer-events: none;
      }
      @media (max-width: 480px) {
        #main { padding: 1.6rem 0.8rem; }
        .article-wrapper { padding: 0.9rem; }
        .article-wrapper .close { right: 0.8rem; top: 0.8rem; width: 32px; height: 32px; line-height: 32px; }
        body { -webkit-text-size-adjust: 100%; }
      }
      img, iframe, embed, video, svg {
        max-width: 100% !important;
        width: auto !important;
        height: auto !important;
        display: block;
      }
    </style>
</head>
<body class="is-preload">
    <div id="wrapper">
        <header id="header" aria-hidden="true">
            <nav><ul></ul></nav>
        </header>
        <div id="main">
            <article id="modules" class="active article-wrapper">
                <a href="index.html#modules" class="close" aria-label="Back to Modules">Ã—</a>
                <h2 class="major">Deciphering Big Data</h2>
                <span class="image main"><img src="images/big-data-pic.jpg" alt="Deciphering Big Data" /></span>
                <p>The Deciphering Big Data module introduces methods and tools for handling, processing and interpreting large-scale datasets. Topics include distributed storage, data pipelines, basic map-reduce concepts, and practical techniques for cleaning and summarising big data.</p>
                <section>
                  <h3 class="major">Topics & Learning Outcomes</h3>
                  <ul>
                    <li>Understanding distributed storage and processing</li>
                    <li>Designing basic data pipelines</li>
                    <li>Practical techniques for cleaning and summarising large datasets</li>
                    <li>Introductory map-reduce concepts and scaling considerations</li>
                  </ul>
                  <h4 class="minor">Unit 1: Introduction to Big Data Technologies and Data Management</h4>
                  <p>The introductory unit in this module introduced big data and its associated technologies, in addition to concepts which underpin data management and how these are used to manipulate big data in organisations.</p>
                  <p>I was tasked with critically evaluating the rationale behind the Internet of Things (IoT) and assess the opportunities, limitations, risks and challenges associated with this method of large-scale data collection.</p>
                  <p>I participated in a discussion to collaborate with my peers and exchange ideas and conclusions drawn, spanning units 1, 2 and 3. I opted to focus on how IoT can be used in the transportation industry to optimise routing and fleet management. My post can be found <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Unit%201%20Collaboration%20Discussion%20-%20Initial%20Post.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>

                  <h4 class="minor">Unit 2: Introduction to Data Types and Formats</h4>
                  <p>The module's second unit examined different data types and formats and the impact it has on how data is stored in different formats. I also developed my knowledge of the differences between structured, quasi, semi and unstructured data formats, as well as how APIs are used to parse data.</p>
                    
                  <h4 class="minor">Unit 3: Data Collection and Storage</h4>
                  <p>Unit 3 focused heavily on data collection and subsequent storage, using data collection techniques such as web scraping. I was also introduced to the BeautifulSoup library in Python and completed an activity to employ web scraping techniques, retrieving a list of data science-related jobs.</p>
                  <p>Access my coding file for this activity <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Unit%203%20Web%20Scraping%20Activity.ipynb" target="_blank" rel="noopener noreferrer">here</a>.</p>
                  
                  <h4 class="minor">Unit 4: Data Cleaning and Transformation</h4>
                  <p>Unit 4 introduced the different concepts, techniques, and methods of data scleaning and transformation. I developed an understanding of the data management pipeline, factors which can affect data cleaning, and automation of the process.</p>
                  <p>I completed an activity to clean UNICEF's Child Labour datasets using data cleaning techniques discovered in Kazil and Jarmul's (2016) Data Wrangling with Python textbook. Access my coding file for this activity <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Unit%204%20Cleaning%20Lecturecast%20Exercise.ipynb.zip" target="_blank" rel="noopener noreferrer">here</a>.</p>
                  
                  <h4 class="minor">Unit 6: Database Design and Normalisation</h4>
                  <p>During this unit, I evaluated how cleaning methods help with storing usable datasets, learned how databases are created and how it links with the use of key fields, analysed anomalies and how they can affect a database's integrity, and was introduced to normalisation and the rationale behind the use of different normal forms.</p>
                  <p>We were also tasked with completing a group project with our fellow coursemates. Acting as technical consultants, we devised a proposal for a MySQL database for a B2B retail organisation, ElectroSpares, which complied with their requirements. The report can be found <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Group%203%20Database%20Proposal%20Report.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>
                
                  <h4 class="minor">Unit 7: Constructing Normalised Tables and Database Build</h4>
                  <p>This unit focused on normalisation and building databases, introduced in Unit 6. I examined data attributes, associations and relationships, normalised a set of data from a fictional school, constructed a MySQL database based on this data, and tested the databsae to check for errors and anomalies.</p>
                  <p>The coding file where I used MySQL to construct the database can be found <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Unit%207%20Students%20DB.sql" target="_blank" rel="noopener noreferrer">here</a> alongside the <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Unit%207%20-%20Normalisation%20Task%20Table.pdf" target="_blank" rel="noopener noreferrer">data normalisation task</a> and data inputted into the database.</p>

                  <h4 class="minor">Unit 8: Compliance and Regulatory Framework for Managing Data</h4>
                  <p>Unit 8 examined compliance frameworks with regards to data management. I analysed compliance obligations for data stakeholders, examined the rights that individuals possess with regards to how their data is used, applicable regulatory frameworks, evaluate activities which require regulation, and reviewed how different organisations are affected by regulation.</p>
                  <p>I participated in a second discussion with my peers, focusing on comparing the different compliance laws which exist across the globe. This post spanned units 8, 9 and 10. My post can be found here.</p>

                  <h4 class="minor">Unit 9: Database Management Systems (DBMS) and Models</h4>
                  <p>This unit explored concepts and theories which underpin Database Management Systems (DBMS), including flat files, relational databases (e.g., SQL, PostgreSQL, MySQL) and non-relational databases (e.g., MongoDB), in addition to data warehousing concepts and data lakes.</p>

                  <h4 class="minor">Unit 10: More on Application Programming Interfaces (APIs) for Data Parsing</h4>
                  <p>I analysed and evaluated APIs, and discovered how APIs facilitate data parsing and inter-process communication. I also reviewed the security requirements of APIs and considered the challenges and issues involved with them.</p>
                  <p>I created a brief security requirements specification for the NHS' Ambulance Data Submission API, which can be found <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Unit%2010%20-%20API%20Security%20Requirements.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>

                  <h4 class="minor">Unit 11: DBMS Transaction and Recovery</h4>
                  <p>In this unit, I explored how databases systems can fail and reviewed which systems are in place to protect against the loss of data, as well as achieving consistency in a database to ensure transactions either fully commit, or not at all, to reduce errors within the database.</p>
                  <p>I completed a critical evaluation of a commonly used disaster recovery procedure, Grandfather-Father-Son (GFS) backup procedure, <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/U11%20-%20GFS%20Backup%20Evaluation.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>
                  <p>The module's penultimate assignment was to complete an executive summary of the design and build of the database proposed in the Unit 6 assignment. This report can be found <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Unit%2011%20-%20Executive%20Summary%20Report.pdf" target="_blank" rel="noopener noreferrer">here</a>. I also completed an evaluation, comparing this summary against the original proposal completed in unit 6, accessed <a href="https://github.com/liawilliams/liawilliams.github.io/blob/main/Deciphering%20Big%20Data/Evaluation%20of%20the%20Unit%206%20Assignment%20vs%20Unit%2011%20Assignment.pdf" target="_blank" rel="noopener noreferrer">here</a>.</p>

                  <h4 class="minor">Unit 12: The Future of Big Data Analytics</h4>
                  <p>The final unit presented how machine learning can be used to advance the analysis of big data. It also revisited compliance frameworks, including regulations and laws, to protect personal data.</p>
                  <p>I was tasked with completing a personal reflection, reviewing my progress across this module and how my technical and professional skills have been developed. This reflection can be found here.</p>
                </section>
            </article>
        </div>
        <footer id="footer"></footer>
    </div>
    <div id="bg"></div>
    <script>
      (function() {
        window.addEventListener('load', function() {
          document.body.classList.remove('is-preload');
          const art = document.querySelector('.article-wrapper');
          setTimeout(function(){ art && art.classList.add('visible'); }, 80);
        });
      })();
    </script>
</body>
</html>

